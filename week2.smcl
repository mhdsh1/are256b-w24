{smcl}
{com}{sf}{ul off}{txt}{.-}
      name:  {res}<unnamed>
       {txt}log:  {res}C:\Users\mahdi\are256b-w24\week2.smcl
  {txt}log type:  {res}smcl
 {txt}opened on:  {res}19 Jan 2024, 09:54:28
{txt}
{com}. *--------------------------------------------------
. 
. 
. *open a .dta (Stata) file
. *we use clear to reaplce the new dataset with the former one
. use "data\EAWE01.dta", clear 
{txt}
{com}. 
. *--------------------------------------------------
. *linear model
. *--------------------------------------------------
. *let us work with some linear probability models
. *P(Y_i=1|X_i) = \beta X_i + \epsilon_i 
. *Prob of finishing a bachelor's degree vs composite cognitive ability test
. 
. reg EDUCBA  ASVABC, robust

{txt}Linear regression                               Number of obs     = {res}       500
                                                {txt}F(1, 498)         =  {res}    87.02
                                                {txt}Prob > F          = {res}    0.0000
                                                {txt}R-squared         = {res}    0.1185
                                                {txt}Root MSE          =    {res} .42946

{txt}{hline 13}{c TT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{col 14}{c |}{col 26}    Robust
{col 1}      EDUCBA{col 14}{c |}      Coef.{col 26}   Std. Err.{col 38}      t{col 46}   P>|t|{col 54}     [95% Con{col 67}f. Interval]
{hline 13}{c +}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{space 6}ASVABC {c |}{col 14}{res}{space 2} .1746469{col 26}{space 2} .0187219{col 37}{space 1}    9.33{col 46}{space 3}0.000{col 54}{space 4} .1378633{col 67}{space 3} .2114305
{txt}{space 7}_cons {c |}{col 14}{res}{space 2} .2566462{col 26}{space 2}  .017902{col 37}{space 1}   14.34{col 46}{space 3}0.000{col 54}{space 4} .2214734{col 67}{space 3}  .291819
{txt}{hline 13}{c BT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{res}{txt}
{com}. 
. * calcualting the \hat{c -(}Y{c )-}_i = \hat{c -(}\beta{c )-}X_i for some values of X_i
. 
. sum ASVABC, detail

                           {txt}ASVABC
{hline 61}
      Percentiles      Smallest
 1%    {res}-2.218827      -3.053471
{txt} 5%    {res}-1.464511      -2.505618
{txt}10%    {res}-.9897522      -2.462728       {txt}Obs         {res}        500
{txt}25%    {res}-.2895314      -2.348369       {txt}Sum of Wgt. {res}        500

{txt}50%    {res}  .334199                      {txt}Mean          {res} .2253335
                        {txt}Largest       Std. Dev.     {res} .9005283
{txt}75%    {res} .8584125       2.000001
{txt}90%    {res} 1.320404       2.003199       {txt}Variance      {res} .8109511
{txt}95%    {res} 1.619688       2.049761       {txt}Skewness      {res}-.4774249
{txt}99%    {res} 1.971871       2.319522       {txt}Kurtosis      {res} 3.170781
{txt}
{com}. 
. display 0.2566+0.1746*0.3341
{res}.31493386
{txt}
{com}. display 0.2566+0.1746*1.9718
{res}.60087628
{txt}
{com}. display 0.2566+0.1746*(-2.2188)
{res}-.13080248
{txt}
{com}. 
. * alternative way to calculate the predicted probability
. display _b[_cons]+_b[ASVABC]*0.3341
{res}.31499574
{txt}
{com}. display _b[_cons]+_b[ASVABC]*1.9718
{res}.60101499
{txt}
{com}. display _b[_cons]+_b[ASVABC]*(-2.2188)
{res}-.13086036
{txt}
{com}. 
. *Does the last predicted probability make sense? 
. * No, it yields a negative probability
. 
. *let's find the fitted values for all the observations
. *\hat{c -(}Y{c )-}_i = \hat{c -(}\beta{c )-}X_i
. *command predict yields the fitted values for all the observations 
. * based on the "latest" model ran 
. help predict // like ? predict in R
{txt}
{com}. predict EDUCBA_hat, xb  
{txt}
{com}. 
. browse EDUCBA EDUCBA_hat
{txt}
{com}. 
. count if EDUCBA_hat>1
  {res}0
{txt}
{com}. count if EDUCBA_hat<0
  {res}24
{txt}
{com}. count if missing(EDUCBA_hat)
  {res}0
{txt}
{com}. 
. *Show the predicted probability graphically
. twoway scatter EDUCBA_hat ASVABC
{res}{txt}
{com}. graph export outputs/linear.png, replace
{txt}(file outputs/linear.png written in PNG format)

{com}. 
. *--------------------------------------------------
. *nonlinear model
. *--------------------------------------------------
. 
. *let us move to non-linear probability models
. *Non-linear probability models map the dependent variables using a function
. *whose range lies between zero and one.
. 
. *PROBIT: The function used for mapping is the cumulative distribution
. *of a normal.
. *P(Y_i=1|X_i) = \Phi(\beta X_i + \epsilon_i) 
. 
. probit EDUCBA  ASVABC, robust 

{res}{txt}Iteration 0:{space 3}log pseudolikelihood = {res:-303.71846}  
Iteration 1:{space 3}log pseudolikelihood = {res:-270.33421}  
Iteration 2:{space 3}log pseudolikelihood = {res:-269.96199}  
Iteration 3:{space 3}log pseudolikelihood = {res:-269.96172}  
Iteration 4:{space 3}log pseudolikelihood = {res:-269.96172}  
{res}
{txt}Probit regression{col 49}Number of obs{col 67}= {res}       500
{txt}{col 49}Wald chi2({res}1{txt}){col 67}= {res}     59.05
{txt}{col 49}Prob > chi2{col 67}= {res}    0.0000
{txt}Log pseudolikelihood = {res}-269.96172{txt}{col 49}Pseudo R2{col 67}= {res}    0.1111

{txt}{hline 13}{c TT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{col 14}{c |}{col 26}    Robust
{col 1}      EDUCBA{col 14}{c |}      Coef.{col 26}   Std. Err.{col 38}      z{col 46}   P>|z|{col 54}     [95% Con{col 67}f. Interval]
{hline 13}{c +}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{space 6}ASVABC {c |}{col 14}{res}{space 2} .6190642{col 26}{space 2} .0805616{col 37}{space 1}    7.68{col 46}{space 3}0.000{col 54}{space 4} .4611664{col 67}{space 3} .7769619
{txt}{space 7}_cons {c |}{col 14}{res}{space 2}-.7621472{col 26}{space 2} .0707757{col 37}{space 1}  -10.77{col 46}{space 3}0.000{col 54}{space 4}-.9008652{col 67}{space 3}-.6234293
{txt}{hline 13}{c BT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}

{com}. *Compare results with Linear Probability Model
. 
. *--------------------------------------------------
.         
. *Computing marginal effects
. *Look at Slides 68-69 for definitions
. *Average Margnal Effect
. margins, dydx(ASVABC)
{res}
{txt}Average marginal effects{col 49}Number of obs{col 67}= {res}       500
{txt}Model VCE{col 14}: {res}Robust

{txt}{p2colset 1 14 16 2}{...}
{p2col:Expression}:{space 1}{res:Pr(EDUCBA), predict()}{p_end}
{p2colreset}{...}
{txt}{p2colset 1 14 16 2}{...}
{p2col:dy/dx w.r.t.}:{space 1}{res:ASVABC}{p_end}
{p2colreset}{...}

{res}{txt}{hline 13}{c TT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{col 14}{c |}{col 26} Delta-method
{col 14}{c |}      dy/dx{col 26}   Std. Err.{col 38}      z{col 46}   P>|z|{col 54}     [95% Con{col 67}f. Interval]
{hline 13}{c +}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{space 6}ASVABC {c |}{col 14}{res}{space 2} .1888049{col 26}{space 2} .0206424{col 37}{space 1}    9.15{col 46}{space 3}0.000{col 54}{space 4} .1483465{col 67}{space 3} .2292632
{txt}{hline 13}{c BT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{res}{txt}
{com}. 
. *Marginal effects evaluated at the mean 
. margins, dydx(ASVABC) atmeans
{res}
{txt}Conditional marginal effects{col 49}Number of obs{col 67}= {res}       500
{txt}Model VCE{col 14}: {res}Robust

{txt}{p2colset 1 14 16 2}{...}
{p2col:Expression}:{space 1}{res:Pr(EDUCBA), predict()}{p_end}
{p2colreset}{...}
{txt}{p2colset 1 14 16 2}{...}
{p2col:dy/dx w.r.t.}:{space 1}{res:ASVABC}{p_end}
{p2colreset}{...}
{txt}{p2colset 1 14 16 2}{...}
{p2col:at}:{space 1}{res:{txt:ASVABC}{space 10}{txt:=} {space 3}.2253335 {txt:(mean)}}{p_end}
{p2colreset}{...}

{res}{txt}{hline 13}{c TT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{col 14}{c |}{col 26} Delta-method
{col 14}{c |}      dy/dx{col 26}   Std. Err.{col 38}      z{col 46}   P>|z|{col 54}     [95% Con{col 67}f. Interval]
{hline 13}{c +}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{space 6}ASVABC {c |}{col 14}{res}{space 2} .2034506{col 26}{space 2} .0257674{col 37}{space 1}    7.90{col 46}{space 3}0.000{col 54}{space 4} .1529474{col 67}{space 3} .2539537
{txt}{hline 13}{c BT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{res}{txt}
{com}. // alternative: mfx compute, dydx
. 
. *Marginal effects evaluated at a different point
. margins, dydx(ASVABC) at(ASVABC=0.1)
{res}
{txt}Conditional marginal effects{col 49}Number of obs{col 67}= {res}       500
{txt}Model VCE{col 14}: {res}Robust

{txt}{p2colset 1 14 16 2}{...}
{p2col:Expression}:{space 1}{res:Pr(EDUCBA), predict()}{p_end}
{p2colreset}{...}
{txt}{p2colset 1 14 16 2}{...}
{p2col:dy/dx w.r.t.}:{space 1}{res:ASVABC}{p_end}
{p2colreset}{...}
{txt}{p2colset 1 14 16 2}{...}
{p2col:at}:{space 1}{res:{txt:ASVABC}{space 10}{txt:=} {space 9}.1}{p_end}
{p2colreset}{...}

{res}{txt}{hline 13}{c TT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{col 14}{c |}{col 26} Delta-method
{col 14}{c |}      dy/dx{col 26}   Std. Err.{col 38}      z{col 46}   P>|z|{col 54}     [95% Con{col 67}f. Interval]
{hline 13}{c +}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{space 6}ASVABC {c |}{col 14}{res}{space 2} .1932726{col 26}{space 2} .0232853{col 37}{space 1}    8.30{col 46}{space 3}0.000{col 54}{space 4} .1476342{col 67}{space 3} .2389111
{txt}{hline 13}{c BT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{res}{txt}
{com}. margins, dydx(ASVABC) at(ASVABC=0.6)
{res}
{txt}Conditional marginal effects{col 49}Number of obs{col 67}= {res}       500
{txt}Model VCE{col 14}: {res}Robust

{txt}{p2colset 1 14 16 2}{...}
{p2col:Expression}:{space 1}{res:Pr(EDUCBA), predict()}{p_end}
{p2colreset}{...}
{txt}{p2colset 1 14 16 2}{...}
{p2col:dy/dx w.r.t.}:{space 1}{res:ASVABC}{p_end}
{p2colreset}{...}
{txt}{p2colset 1 14 16 2}{...}
{p2col:at}:{space 1}{res:{txt:ASVABC}{space 10}{txt:=} {space 9}.6}{p_end}
{p2colreset}{...}

{res}{txt}{hline 13}{c TT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{col 14}{c |}{col 26} Delta-method
{col 14}{c |}      dy/dx{col 26}   Std. Err.{col 38}      z{col 46}   P>|z|{col 54}     [95% Con{col 67}f. Interval]
{hline 13}{c +}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{space 6}ASVABC {c |}{col 14}{res}{space 2} .2288218{col 26}{space 2} .0315965{col 37}{space 1}    7.24{col 46}{space 3}0.000{col 54}{space 4} .1668938{col 67}{space 3} .2907499
{txt}{hline 13}{c BT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{res}{txt}
{com}. 
. 
. // what does margins alone do?
. 
. *Predict Probability
. *\hat{c -(}Y{c )-}_i = \Phi{c -(}\hat{c -(}\beta{c )-}X_i{c )-}
. 
. *calculating the predicted probability
. h nlcom
{txt}
{com}. h norm
{txt}
{com}. *At 75 percentile
. nlcom norm(_b[ASVABC]*0.8584 + _b[ _cons])

       {txt}_nl_1:  {res}norm(_b[ASVABC]*0.8584 + _b[ _cons])

{txt}{hline 13}{c TT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{col 1}      EDUCBA{col 14}{c |}      Coef.{col 26}   Std. Err.{col 38}      z{col 46}   P>|z|{col 54}     [95% Con{col 67}f. Interval]
{hline 13}{c +}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{space 7}_nl_1 {c |}{col 14}{res}{space 2} .4087574{col 26}{space 2} .0278879{col 37}{space 1}   14.66{col 46}{space 3}0.000{col 54}{space 4} .3540982{col 67}{space 3} .4634166
{txt}{hline 13}{c BT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}

{com}. *At 1 percentile
. nlcom norm(_b[ASVABC]*-2.2188 + _b[ _cons])

       {txt}_nl_1:  {res}norm(_b[ASVABC]*-2.2188 + _b[ _cons])

{txt}{hline 13}{c TT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{col 1}      EDUCBA{col 14}{c |}      Coef.{col 26}   Std. Err.{col 38}      z{col 46}   P>|z|{col 54}     [95% Con{col 67}f. Interval]
{hline 13}{c +}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{space 7}_nl_1 {c |}{col 14}{res}{space 2} .0163508{col 26}{space 2} .0090219{col 37}{space 1}    1.81{col 46}{space 3}0.070{col 54}{space 4}-.0013318{col 67}{space 3} .0340335
{txt}{hline 13}{c BT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}

{com}. 
. *Generate variable that predicts for every observation
. predict EDUCBA_probit_hat
{txt}(option {bf:pr} assumed; Pr(EDUCBA))

{com}. browse EDUCBA EDUCBA_hat EDUCBA_probit_hat
{txt}
{com}. twoway (scatter EDUCBA_probit_hat ASVABC)
{res}{txt}
{com}. 
. *--------------------------------------------------
. *model comparison based on rmse
. *--------------------------------------------------
. *How do the models compare? (linear vs probit)
. twoway (scatter EDUCBA_probit_hat ASVABC) ///
>        (scatter EDUCBA_hat ASVABC) ///
>        (scatter EDUCBA ASVABC)
{res}{txt}
{com}. 
. *We use root mean squared error (rmse) concept to compare 
. *rmse = sqrt{c -(}((1/n)*(\Sigma{c -(}(Y_i - \hat{c -(}Y_i{c )-})^2{c )-}){c )-}
. * look at slide 65
. 
. gen sqerror        = (EDUCBA - EDUCBA_hat)^2
{txt}
{com}. gen sqerror_probit = (EDUCBA - EDUCBA_probit_hat)^2
{txt}
{com}. 
. 
. qui summarize sqerror 
{txt}
{com}. di r(mean)^0.5
{res}.42860029
{txt}
{com}. 
. qui summarize sqerror_probit
{txt}
{com}. di r(mean)^0.5
{res}.42795955
{txt}
{com}. 
. *--------------------------------------------------
. *Cenosred data and the Tobit model
. *--------------------------------------------------
. 
. *Censored Data Generation (Monte Carlo Method):
. clear all
{txt}
{com}. set obs 50
{txt}{p}
number of observations (_N)  was 0,
now 50
{p_end}

{com}. gen X=_n+10
{txt}
{com}. gen U=rnormal(0,10)
{txt}
{com}. gen Ystar=-40+1.2*X+U
{txt}
{com}. gen Y= Ystar*(Ystar>0)
{txt}
{com}. 
. scatter Y X if Y>0 || lfit Y X if Y>0|| lfit Ystar X, ///
> legend(label(1 "Y")  ///
> label(2 "Truncated Regression")  ///
> label(3 "True Regression Relationship") )
{res}{txt}
{com}. 
. regress Y X if Y>0

{txt}      Source {c |}       SS           df       MS      Number of obs   ={res}        28
{txt}{hline 13}{c +}{hline 34}   F(1, 26)        = {res}    35.23
{txt}       Model {c |} {res} 1835.40834         1  1835.40834   {txt}Prob > F        ={res}    0.0000
{txt}    Residual {c |} {res} 1354.44646        26  52.0940947   {txt}R-squared       ={res}    0.5754
{txt}{hline 13}{c +}{hline 34}   Adj R-squared   ={res}    0.5591
{txt}       Total {c |} {res}  3189.8548        27   118.14277   {txt}Root MSE        =   {res} 7.2176

{txt}{hline 13}{c TT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{col 1}           Y{col 14}{c |}      Coef.{col 26}   Std. Err.{col 38}      t{col 46}   P>|t|{col 54}     [95% Con{col 67}f. Interval]
{hline 13}{c +}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{space 11}X {c |}{col 14}{res}{space 2} .8690103{col 26}{space 2}  .146404{col 37}{space 1}    5.94{col 46}{space 3}0.000{col 54}{space 4} .5680726{col 67}{space 3} 1.169948
{txt}{space 7}_cons {c |}{col 14}{res}{space 2}-24.52125{col 26}{space 2} 6.820086{col 37}{space 1}   -3.60{col 46}{space 3}0.001{col 54}{space 4}-38.54013{col 67}{space 3}-10.50236
{txt}{hline 13}{c BT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{res}{txt}
{com}. 
. *the truncated regression slope is biased (slide 77)
. *one soltion is Tobit model
. *ll() argument isleft-censoring limit i.e. 
. * We only observe Y_i > 0 in this regression. 
. tobit Y X, ll(0)  robust
{res}
{txt}Tobit regression{col 49}Number of obs{col 67}= {res}        50
{txt}{col 49}F({res}   1{txt},{res}     49{txt}){col 67}= {res}     87.74
{txt}{col 49}Prob > F{col 67}= {res}    0.0000
{txt}Log pseudolikelihood = {res}-101.72532{txt}{col 49}Pseudo R2{col 67}= {res}    0.2536

{txt}{hline 13}{c TT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{col 14}{c |}{col 26}    Robust
{col 1}           Y{col 14}{c |}      Coef.{col 26}   Std. Err.{col 38}      t{col 46}   P>|t|{col 54}     [95% Con{col 67}f. Interval]
{hline 13}{c +}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{space 11}X {c |}{col 14}{res}{space 2} 1.099829{col 26}{space 2} .1174126{col 37}{space 1}    9.37{col 46}{space 3}0.000{col 54}{space 4} .8638792{col 67}{space 3} 1.335778
{txt}{space 7}_cons {c |}{col 14}{res}{space 2}-36.71956{col 26}{space 2} 5.270491{col 37}{space 1}   -6.97{col 46}{space 3}0.000{col 54}{space 4}-47.31101{col 67}{space 3}-26.12811
{txt}{hline 13}{c +}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
      /sigma {c |}{col 14}{res}{space 2} 7.493086{col 26}{space 2} 1.059238{col 54}{space 4} 5.364468{col 67}{space 3} 9.621704
{txt}{hline 13}{c BT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{res}            22{txt}  left-censored observations at Y <= {res}0
            28{txt}     uncensored observations
{res}             0{txt} right-censored observations

{com}. 
. 
. *--------------------------------------------------
. * presentation: exporting tables
. *--------------------------------------------------
. /*
> *use estout to generate nice tables
> ssc install estout, replace
> 
> *To create nice LATEX/Doc tables we can use this command
> *If you do not want/need Latex output, just erase the commands.
> eststo clear
> eststo model_l: quietly regress EDUCBA  ASVABC, robust 
> eststo model_p: quietly probit EDUCBA  ASVABC, robust 
> 
> esttab model_l
> 
> 
> esttab model_l using outputs/model_l.rtf, replace ///
> se onecell width(\hsize) ///
> addnote() ///
> label title(Estimation Result of Linear Model)
> */
. *--------------------------------------------------
.  
. log close // Close the log, end the file
      {txt}name:  {res}<unnamed>
       {txt}log:  {res}C:\Users\mahdi\are256b-w24\week2.smcl
  {txt}log type:  {res}smcl
 {txt}closed on:  {res}19 Jan 2024, 09:54:38
{txt}{.-}
{smcl}
{txt}{sf}{ul off}